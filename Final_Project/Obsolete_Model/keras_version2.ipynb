{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution1D mainly used for NLP\n",
    "#### Convolution2D mainly used for CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\butte\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils, conv_utils\n",
    "# from keras.utils.conv_utils import normalize_data_format\n",
    "# normalize_data_format function was moved to keras.backend.common from keras.utils.conv_utils since keras 2.2.1\n",
    "from keras.backend.common import normalize_data_format\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dropout, Dense,Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of different kinds of fruits\n",
    "\n",
    "def get_name_list(filepath):\n",
    "    pathDir = os.listdir(filepath)\n",
    "    out = []\n",
    "    for allDir in pathDir:\n",
    "        if os.path.isdir(os.path.join(filepath,allDir)):\n",
    "            child = allDir.decode('gbk')\n",
    "            out.append(child)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the names of all folders and files in a list\n",
    "\n",
    "def eachFile(filepath):\n",
    "    pathDir = os.listdir(filepath)\n",
    "    out = []\n",
    "    for allDir in pathDir:\n",
    "        child = allDir\n",
    "        out.append(child)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get_data used to convert target image dataset to numpy array\n",
    "\n",
    "def get_data(data_name, train_percentage=0.7, resize=True,data_format=None):\n",
    "    file_name = os.path.join(pic_dir_out, data_name+str(Width)+\"X\"+str(Height)+\".pkl\")\n",
    "    if os.path.exists(file_name):\n",
    "        (X_train, y_train),(X_test, y_test)= pickle.load(open(file_name, \"rb\"))\n",
    "        return (X_train, y_train),(X_test, y_test)\n",
    "#     data_format = conv_utils.normalize_data_format(data_format)\n",
    "    data_format = normalize_data_format(data_format)\n",
    "    pic_dir_set = eachFile(pic_dir_data) \n",
    "    # pic_dir_data is the path of our dataset\n",
    "    # pic_dir_set includes all the folders in the dataset path(for example apple, banana etc.)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    label = 0\n",
    "    for pic_dir in pic_dir_set:\n",
    "        print(pic_dir_data+pic_dir)# print the current folder name\n",
    "        if not os.path.isdir(os.path.join(pic_dir_data, pic_dir)):\n",
    "            continue \n",
    "        pic_set = eachFile(os.path.join(pic_dir_data, pic_dir))\n",
    "        # pic_set contains all the images in the current folder\n",
    "        pic_index = 0\n",
    "        train_count = int(len(pic_set)*train_percentage)\n",
    "        for pic_name in pic_set:\n",
    "            if not os.path.isfile(os.path.join(pic_dir_data, pic_dir, pic_name)):\n",
    "                continue\n",
    "            img = cv2.imread(os.path.join(pic_dir_data, pic_dir, pic_name))\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            # RGB to gray scale\n",
    "            if(resize):\n",
    "                img = cv2.resize(img,(Width, Height))\n",
    "            if(data_format == 'channels_last'):\n",
    "                img = img.reshape(-1,Width, Height,1)\n",
    "            elif(data_format == 'channels_first'):\n",
    "                img = img.reshape(-1,1,Width,Height)\n",
    "            if(pic_index < train_count):\n",
    "                X_train.append(img)\n",
    "                y_train.append(label)\n",
    "            else:\n",
    "                X_test.append(img)\n",
    "                y_test.append(label)\n",
    "            pic_index += 1\n",
    "        if len(pic_set)!= 0:\n",
    "            label += 1\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    X_test = np.concatenate(X_test, axis=0)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    pickle.dump([(X_train, y_train), (X_test, y_test)], open(file_name, 'wb'))\n",
    "    return(X_train, y_train), (X_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global Width, Height, pic_dir_out, pic_dir_data\n",
    "    Width=100\n",
    "    Height=100\n",
    "    num_classes=3\n",
    "    pic_dir_out='C:/Users/butte/Jupyter/Final_Project/pic/pic_out'\n",
    "    pic_dir_data='C:/Users/butte/Jupyter/Final_Project/pic/pic_dataset'\n",
    "    (X_train, y_train),(X_test,y_test)= get_data(\"grey_data_\",0.7,data_format='channels_last')\n",
    "    print(\"read and transform images successfully\")\n",
    "    \n",
    "    \n",
    "    # data preprocessing_normalization\n",
    "    X_train = X_train/255. #normalization to (0,1) in order to fit in the neural network\n",
    "    X_test = X_test/255.\n",
    "    print('X_train.shape after normalization:', X_train.shape)\n",
    "    print('X_test.shape after normalization:', X_test.shape)\n",
    "\n",
    "    # num_classes needed to be given\n",
    "    # turn the specific number of a certain kind to a multidimension array\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    ########################################################\n",
    "    # build CNN model\n",
    "    model = Sequential()\n",
    "          \n",
    "    # Convolution\n",
    "    # first convolution layer\n",
    "    model.add(Convolution2D(input_shape=(Height,Width ,1),\n",
    "                           filters=32,\n",
    "                           kernel_size=(3,3),\n",
    "                           strides=(1,1),\n",
    "                           padding='same',\n",
    "                           data_format='channels_last',# default data_format\n",
    "                           ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size=(3,3),\n",
    "        strides=(3,3),\n",
    "        data_format='channels_last',\n",
    "    ))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # second convolution layer (CONV->RELU)*2 ->POOL\n",
    "    model.add(Convolution2D(64,(3,3),strides=(1,1),padding='same',data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(64,(3,3),strides=(1,1),padding='same',data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D((2,2),(2,2),data_format='channels_last'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # third convolution layer\n",
    "    model.add(Convolution2D(128,(3,3),strides=(1,1),padding='same',data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Convolution2D(128,(3,3),strides=(1,1),padding='same',data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D((2,2),(2,2),data_format='channels_last'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten Dense Dropout\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    #softmax convert the prediciton result to (0,1)\n",
    "    \n",
    "    model.compile(optimizer=Adam(),#method to optimize hyperparameters\n",
    "                 loss='categorical_crossentropy',# method to calculate loss\n",
    "                 metrics=['accuracy']) # method to evaluate model\n",
    "    \n",
    "    #########################################\n",
    "    print(\"\\nTraining----------------------------\")\n",
    "    cm=0\n",
    "    cm_str=''if cm==0 else str(cm)\n",
    "    cm2_str='' if (cm+1)==0 else str(cm+1)\n",
    "    if(cm>=1):\n",
    "        model.load_weights(os.path.join(pic_dir_out, 'cnn_model_'+cm_str+'.h5'))\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=128)\n",
    "    model.save_weights(os.path.join(pic_dir_out,'cnn_model_'+cm2_str+'.h5'))\n",
    "    model.save(os.path.join(pic_dir_out, 'my_model.h5'))\n",
    "    \n",
    "    # try to save model\n",
    "#     f_name='C:/Users/butte/Jupyter/Final_Project/pic/pic_out/finalized_model.sav'\n",
    "#     pickle.dump(model, open(f_name,'wb'))\n",
    "    \n",
    "    ########################################\n",
    "    print(\"\\nTesting----------------------------\")              \n",
    "    loss,accuracy= model.evaluate(X_test, y_test)\n",
    "    print('test loss:', loss)\n",
    "    print('test accuracy:', accuracy)\n",
    "\n",
    "#     class_name_list = get_name_list(pic_dir_data) #list the name of all kinds of fruits in the dataset directory\n",
    "#     predict = model.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在构建网络模型的时候，给每一层都定义一个名字，这样在复用之前的参数权重的时候，除了官网给的先加载权重，再冻结权重之外，你可以通过简单的修改层的名字来达到加载之前训练的权重的目的。save_weights将每一次迭代后的模型参数保存下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/butte/Jupyter/Final_Project/pic/pic_datasetapple\n",
      "C:/Users/butte/Jupyter/Final_Project/pic/pic_datasetbanana\n",
      "C:/Users/butte/Jupyter/Final_Project/pic/pic_datasetorange\n",
      "read and transform images successfully\n",
      "X_train.shape after normalization: (153, 100, 100, 1)\n",
      "X_test.shape after normalization: (67, 100, 100, 1)\n",
      "\n",
      "Training----------------------------\n",
      "Epoch 1/50\n",
      "153/153 [==============================] - 29s 186ms/step - loss: 1.9074 - acc: 0.4183\n",
      "Epoch 2/50\n",
      "153/153 [==============================] - 24s 154ms/step - loss: 1.8515 - acc: 0.7516\n",
      "Epoch 3/50\n",
      "153/153 [==============================] - 23s 148ms/step - loss: 1.1897 - acc: 0.7974\n",
      "Epoch 4/50\n",
      "153/153 [==============================] - 23s 150ms/step - loss: 1.1005 - acc: 0.7908\n",
      "Epoch 5/50\n",
      "153/153 [==============================] - 22s 142ms/step - loss: 0.8128 - acc: 0.8366\n",
      "Epoch 6/50\n",
      "153/153 [==============================] - 21s 136ms/step - loss: 0.5518 - acc: 0.8758\n",
      "Epoch 7/50\n",
      "153/153 [==============================] - 21s 139ms/step - loss: 0.5792 - acc: 0.8954\n",
      "Epoch 8/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.6584 - acc: 0.8170\n",
      "Epoch 9/50\n",
      "153/153 [==============================] - 23s 153ms/step - loss: 0.5135 - acc: 0.8758\n",
      "Epoch 10/50\n",
      "153/153 [==============================] - 24s 160ms/step - loss: 0.3121 - acc: 0.9346\n",
      "Epoch 11/50\n",
      "153/153 [==============================] - 25s 161ms/step - loss: 0.2176 - acc: 0.9412\n",
      "Epoch 12/50\n",
      "153/153 [==============================] - 23s 153ms/step - loss: 0.2061 - acc: 0.9412\n",
      "Epoch 13/50\n",
      "153/153 [==============================] - 23s 152ms/step - loss: 0.2048 - acc: 0.9412\n",
      "Epoch 14/50\n",
      "153/153 [==============================] - 23s 152ms/step - loss: 0.1227 - acc: 0.9739\n",
      "Epoch 15/50\n",
      "153/153 [==============================] - 22s 146ms/step - loss: 0.0880 - acc: 0.9608\n",
      "Epoch 16/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.1955 - acc: 0.9542\n",
      "Epoch 17/50\n",
      "153/153 [==============================] - 22s 144ms/step - loss: 0.1827 - acc: 0.9412\n",
      "Epoch 18/50\n",
      "153/153 [==============================] - 22s 144ms/step - loss: 0.1024 - acc: 0.9608\n",
      "Epoch 19/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0972 - acc: 0.9542\n",
      "Epoch 20/50\n",
      "153/153 [==============================] - 22s 147ms/step - loss: 0.0863 - acc: 0.9608\n",
      "Epoch 21/50\n",
      "153/153 [==============================] - 22s 144ms/step - loss: 0.0847 - acc: 0.9804\n",
      "Epoch 22/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0759 - acc: 0.9673\n",
      "Epoch 23/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0634 - acc: 0.9673\n",
      "Epoch 24/50\n",
      "153/153 [==============================] - 23s 148ms/step - loss: 0.0788 - acc: 0.9739\n",
      "Epoch 25/50\n",
      "153/153 [==============================] - 23s 149ms/step - loss: 0.0458 - acc: 0.9935\n",
      "Epoch 26/50\n",
      "153/153 [==============================] - 24s 158ms/step - loss: 0.0906 - acc: 0.9673\n",
      "Epoch 27/50\n",
      "153/153 [==============================] - 23s 152ms/step - loss: 0.0968 - acc: 0.9608\n",
      "Epoch 28/50\n",
      "153/153 [==============================] - 22s 146ms/step - loss: 0.1476 - acc: 0.9477\n",
      "Epoch 29/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.1023 - acc: 0.9673\n",
      "Epoch 30/50\n",
      "153/153 [==============================] - 22s 146ms/step - loss: 0.0562 - acc: 0.9804\n",
      "Epoch 31/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0797 - acc: 0.9804\n",
      "Epoch 32/50\n",
      "153/153 [==============================] - 22s 143ms/step - loss: 0.0534 - acc: 0.9673\n",
      "Epoch 33/50\n",
      "153/153 [==============================] - 23s 148ms/step - loss: 0.0564 - acc: 0.9673\n",
      "Epoch 34/50\n",
      "153/153 [==============================] - 23s 149ms/step - loss: 0.0861 - acc: 0.9804\n",
      "Epoch 35/50\n",
      "153/153 [==============================] - 22s 144ms/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "153/153 [==============================] - 22s 141ms/step - loss: 0.0305 - acc: 0.9869\n",
      "Epoch 37/50\n",
      "153/153 [==============================] - 23s 149ms/step - loss: 0.0351 - acc: 0.9869\n",
      "Epoch 38/50\n",
      "153/153 [==============================] - 21s 137ms/step - loss: 0.0333 - acc: 0.9935\n",
      "Epoch 39/50\n",
      "153/153 [==============================] - 24s 154ms/step - loss: 0.0307 - acc: 0.9869\n",
      "Epoch 40/50\n",
      "153/153 [==============================] - 22s 146ms/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "153/153 [==============================] - 23s 148ms/step - loss: 0.0207 - acc: 0.9935\n",
      "Epoch 42/50\n",
      "153/153 [==============================] - 22s 147ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "153/153 [==============================] - 22s 144ms/step - loss: 0.0235 - acc: 0.9869\n",
      "Epoch 44/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0479 - acc: 0.9804\n",
      "Epoch 45/50\n",
      "153/153 [==============================] - 23s 152ms/step - loss: 0.0241 - acc: 0.9935\n",
      "Epoch 46/50\n",
      "153/153 [==============================] - 22s 146ms/step - loss: 0.0193 - acc: 0.9935\n",
      "Epoch 47/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0139 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "153/153 [==============================] - 22s 143ms/step - loss: 0.0733 - acc: 0.9804\n",
      "Epoch 49/50\n",
      "153/153 [==============================] - 23s 149ms/step - loss: 0.0390 - acc: 0.9804\n",
      "Epoch 50/50\n",
      "153/153 [==============================] - 22s 145ms/step - loss: 0.0276 - acc: 0.9869\n",
      "\n",
      "Testing----------------------------\n",
      "67/67 [==============================] - 3s 40ms/step\n",
      "test loss: 2.7246360772136433\n",
      "test accuracy: 0.6417910447761194\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('C:/Users/butte/Jupyter/Final_Project/pic/pic_out/grey_data_100X100.pkl',\"rb\")\n",
    "# data = pickle.load(file)\n",
    "# print(data)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_model=open('C:/Users/butte/Jupyter/Final_Project/pic/pic_out/finalized_model.sav',\"rb\")\n",
    "# loaded_model = pickle.load(file_model)\n",
    "\n",
    "# def predict_specific_image(img, model):\n",
    "#     img = imread()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9242069e-24 1.0000000e+00 2.2542204e-17]]\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('C:\\\\Users\\\\butte\\\\Jupyter\\\\Final_Project\\\\pic\\\\pic_pred\\\\banana_1.jpg')\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \n",
    "img = cv2.resize(img,(Width, Height))\n",
    "img = img.reshape(-1,100,100,1)\n",
    "img = img.astype(\"float\")/255.0\n",
    "# img = img_to_array(img)\n",
    "img = np.array(img) \n",
    "model=load_model('C:\\\\Users\\\\butte\\\\Jupyter\\\\Final_Project\\\\pic\\\\pic_out\\\\my_model.h5')\n",
    "pred = model.predict(img)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
