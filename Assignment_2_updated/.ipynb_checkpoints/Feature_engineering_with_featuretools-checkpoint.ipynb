{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### featuretools can synthesize features by creating relationship between them. \n",
    "\n",
    "##### Typically, featuretools is used when we have several different datasets (e.g. several .csv files, or several pandas dataframes), and they should have some inner-relationship similar to relational database.\n",
    "\n",
    "##### different dataframe will be loaded as different EntitySet in featuretools, and we should define an id (similar to pk or fk in database) for each of the dataset\n",
    "\n",
    "##### when we have a relationship between datasets, we can have a Deep Feature Synthesis using featuretools.\n",
    "\n",
    "`for here, we have only one dataset, featuretools is not applicable for this assignment`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TPOT(Tree-based Pipeline Optimization Tool)\n",
    "\n",
    "### installation: \n",
    "1.`pip install deap update_checker tqdm stopit`\n",
    "\n",
    "2.`pip install pywin32` (if python is not installed via Anaconda)\n",
    "\n",
    "\n",
    "3.`pip install tpot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_get_data():\n",
    "    train_data = pd.read_csv('./training.csv')\n",
    "    test_data = pd.read_csv('./testing.csv')\n",
    "    train_data = split_datetime(train_data)\n",
    "    test_data = split_datetime(test_data)\n",
    "    \n",
    "    x_train = catogirical_to_numerical(train_data.drop(['Appliances'], axis = 1))\n",
    "    y_train = train_data['Appliances'].values\n",
    "\n",
    "    x_test = catogirical_to_numerical(test_data.drop(['Appliances'], axis = 1))\n",
    "    y_test = test_data['Appliances'].values\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def catogirical_to_numerical(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "    vec = DictVectorizer(sparse=False, dtype=int)\n",
    "    dc = df.to_dict('records')\n",
    "    result = vec.fit_transform(dc)\n",
    "    return result\n",
    "def split_datetime(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    year = list()\n",
    "    month = list()\n",
    "    day = list()\n",
    "    hour=list()\n",
    "    for i in np.arange(df.count()[0]):\n",
    "        year.append(df['date'][i].year)\n",
    "        month.append(df['date'][i].month)\n",
    "        day.append(df['date'][i].day)\n",
    "        hour.append(df['date'][i].hour)\n",
    "    df['year'] = year\n",
    "    df['month'] = month\n",
    "    df['day'] = day\n",
    "    df['hour'] = hour\n",
    "    df = df.drop(['date'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def cal_errors(y_test,y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rms = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    print('MAE = {}, RMS = {}, R2 = {}, MAPE = {}'.format(mae,rms,r2,mape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python37-32\\lib\\importlib\\_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=300, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -8799.849473934231\n",
      "Generation 2 - Current best internal CV score: -8771.209432220396\n",
      "Generation 3 - Current best internal CV score: -8701.563001635453\n",
      "Generation 4 - Current best internal CV score: -8699.117897982436\n",
      "Generation 5 - Current best internal CV score: -8593.739592487294\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(LassoLarsCV(FastICA(input_matrix, tol=0.25), normalize=False), bootstrap=False, max_features=0.35000000000000003, min_samples_leaf=18, min_samples_split=15, n_estimators=100)\n",
      "MAE = 42.244488857464084, RMS = 6980.216082289274, R2 = 0.32384072634803185, MAPE = 44.79586610281174\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = read_and_get_data()\n",
    "tpot = TPOTRegressor(generations = 5, population_size=50, verbosity=2)\n",
    "tpot.fit(x_train,y_train)\n",
    "y_pred = tpot.predict(x_test)\n",
    "cal_errors(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
